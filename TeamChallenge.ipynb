{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WYJ8UTs5JULk",
    "outputId": "5a862358-d249-4359-cb2d-a86289c457c8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mounted = False\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "def mount_drive():\n",
    "    global mounted\n",
    "    drive.mount(\"/content/drive\", force_remount=True)\n",
    "    mounted = True\n",
    "\n",
    "def open_dataset(filepath):\n",
    "    if IN_COLAB:\n",
    "        filepath = '/content/drive/My Drive/UNI/CE101/{}'.format(filepath)\n",
    "        if not mounted:\n",
    "            mount_drive()\n",
    "    data = pd.read_csv(filepath)\n",
    "    return data\n",
    "\n",
    "def scatter(y, x):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x = train_dataset[x], y = train_dataset[y])\n",
    "    plt.ylabel(y)\n",
    "    plt.xlabel(x)\n",
    "    plt.show()\n",
    "    \n",
    "def boxplot(x,y):\n",
    "    sns.boxplot(x=train_dataset[x].sort_values(), y=train_dataset[y])\n",
    "      \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#this code takes care of unwanted future warnings (https://stackoverflow.com/questions/52640386/how-do-i-solve-the-future-warning-min-groups-self-n-splits-warning-in)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code loads both of the data files provided using the Pandas method pd.read_csv\n",
    "\n",
    "This formats the data into the dataframes datastructure which allows for easy manipulation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = open_dataset(\"train.csv\")\n",
    "test_dataset = open_dataset(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to begin to understand the problem that needs to be solved, it is important to explore and understand the data. The first variable that needs to be identified is the target variable, which is the variable that the machine leaning model will be predicting.\n",
    "\n",
    "\n",
    "# The sale price: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of the sales price column lists some important facts, such as the mininum price being above 0, which is what you'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_dataset['SalePrice'], fit=norm);\n",
    "\n",
    "(mu, sigma) = norm.fit(train_dataset['SalePrice'])\n",
    "\n",
    "plt.legend(['Norm dist. ($\\mu=$ {:2f} and $\\sigma=$ {:2f})'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('SalePrice distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train_dataset['SalePrice'], plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Variables\n",
    "\n",
    "Next, an important step is to categorise the rest of the variables into their different types, numerical or categorical. Using the data description file, they can be catagorised as so:\n",
    " \n",
    "# Catagorical Data: \n",
    "MSSubClass\n",
    "MSZoning\n",
    "Street\n",
    "Alley\n",
    "LotShape\n",
    "LandContour\n",
    "Utilities\n",
    "LotConfig\n",
    "LandSlope\n",
    "Neighborhood\n",
    "Condition1\n",
    "Condition2\n",
    "BldgType\n",
    "HouseStyle\n",
    "OverallQual\n",
    "OverallCond\n",
    "RoofStyle\n",
    "RoofMatl\n",
    "Exterior1st\n",
    "Exterior2nd\n",
    "MasVnrType\n",
    "ExterQual\n",
    "ExterCond\n",
    "Foundation\n",
    "BsmtQual\n",
    "BsmtCond\n",
    "BsmtExposure\n",
    "BsmtFinType1\n",
    "BsmtFinType2\n",
    "Heating\n",
    "HeatingQC\n",
    "CentralAir\n",
    "Electrical\n",
    "KitchenQual\n",
    "Functional\n",
    "FireplaceQu\n",
    "GarageType\n",
    "GarageFinish\n",
    "GarageQual\n",
    "GarageCond\n",
    "PavedDrive\n",
    "PoolQC\n",
    "Fence\n",
    "MiscFeature\n",
    "SaleType\n",
    "SaleCondition\n",
    "\n",
    "# Numerical Data:\n",
    "LotFrontage\n",
    "LotArea\n",
    "YearBuilt\n",
    "YearRemodAdd\n",
    "MasVnrArea\n",
    "BsmtFinSF1\n",
    "BsmtFinSF2\n",
    "BsmtUnfSF\n",
    "TotalBsmtSF\n",
    "1stFlrSF\n",
    "2ndFlrSF\n",
    "LowQualFinSF\n",
    "GrLivArea\n",
    "BsmtFullBath\n",
    "BsmtHalfBath\n",
    "FullBath\n",
    "HalfBath\n",
    "Bedroom\n",
    "KitchenAbvGr\n",
    "TotRmsAvvGrd\n",
    "Fireplaces\n",
    "GarageYrBlt\n",
    "GarageCars\n",
    "GarageArea\n",
    "WoodDeckSF\n",
    "OpenPorchSF\n",
    "EnclosedPorch\n",
    "3SsnPorch\n",
    "ScreenPorch\n",
    "PoolArea\n",
    "MiscVal\n",
    "MoSold\n",
    "YrSold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = train_dataset.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(correlation, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap above is a good way to see the correlation between many different variables. Squares that are closer to white show that the variables are correlated. For example, in this heatmap it is possible to see that Garage cars is very correlated with garage area, which makes sense as the bigger the garage the greater the number of cars that could be fit in.\n",
    "\n",
    "Other variables showing a strong positive correlation is TotalBsmtSf and 1stFlrSqFt\n",
    "\n",
    "# Catagorical Data\n",
    "\n",
    "Using Boxplots it is possible to show how SalePrice is affected by catagorical Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(\"KitchenQual\", \"SalePrice\")\n",
    "boxplot(\"SaleType\", \"Fireplaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ID = train_dataset['Id']\n",
    "test_ID = test_dataset['Id']\n",
    "\n",
    "\n",
    "# Now the ID column can be dropped as it is not necessary for the prediction process\n",
    "train_dataset.drop('Id', axis = 1, inplace = True)\n",
    "test_dataset.drop('Id', axis = 1, inplace = True)\n",
    "\n",
    "print(\"\\nThe train dataset size after dropping Id feature is: \", train_dataset.shape)\n",
    "print(\"\\nThe test dataset size after dropping Id feature is: \", test_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter('SalePrice', 'GrLivArea')\n",
    "scatter('SalePrice', 'TotalBsmtSF')\n",
    "scatter('SalePrice', '1stFlrSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these graphs it can be seen that there is two major outliers, with a large living area with low price. These can be removed from the dataset. It also could be said that the two values with sales price > 700000 could be outliers, however these follow the trend line so should remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop(train_dataset[(train_dataset['GrLivArea']>4000) & (train_dataset['SalePrice']<300000)].index)\n",
    "\n",
    "# Ensure these have been deleted\n",
    "scatter('SalePrice', 'GrLivArea')\n",
    "scatter('SalePrice', 'TotalBsmtSF')\n",
    "scatter('SalePrice', '1stFlrSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we saw that the target variable (y) is positively skewed. Linear models prefer to work with normally distributed data, therefore we must transform it. A log function is a useful way to transform positively skewed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset[\"SalePrice\"] = np.log1p(train_dataset[\"SalePrice\"])\n",
    "\n",
    "#Check to see if the data is now normally skwewed\n",
    "sns.distplot(train_dataset['SalePrice'], fit=norm);\n",
    "\n",
    "(mu, sigma) = norm.fit(train_dataset['SalePrice'])\n",
    "\n",
    "plt.legend(['Norm dist. ($\\mu=$ {:2f} and $\\sigma=$ {:2f})'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('SalePrice distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train_dataset['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now closer to being normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before begining the data cleanup process, it is a good idea to concatenate the two datasets (test and train) so that any operations applied is applied to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_dataset.SalePrice.values\n",
    "\n",
    "combined_data = pd.concat((train_dataset, test_dataset), keys=[\"train\", \"test\"], sort=False)\n",
    "combined_data.drop(['SalePrice'], axis=1, inplace = True)\n",
    "print(\"combined_data size is:\", combined_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determines the missing values from the test dataset\n",
    "# and shows which columns have at least one missing value.\n",
    "total = combined_data.isnull().sum().sort_values(ascending = False) [combined_data.isnull().sum().sort_values(ascending = False) != 0]\n",
    "percent = round(combined_data.isnull().sum().sort_values(ascending = False)/len(combined_data)*100,2)[round(combined_data.isnull().sum().sort_values(ascending = False)/len(combined_data)*100,2) != 0]\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=missing_data.index, y = missing_data['Percent'])\n",
    "plt.xlabel('Features', fontsize = 15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Misc Feature: data description says NA means no misc features\n",
    "combined_data[\"MiscFeature\"] = combined_data[\"MiscFeature\"].fillna(\"None\")\n",
    "#Alley: data description says NA means \"no alley access\"\n",
    "combined_data[\"Alley\"] = combined_data[\"Alley\"].fillna(\"None\")\n",
    "#Fence: data description says NA means no fence\n",
    "combined_data[\"Fence\"] = combined_data[\"Fence\"].fillna(\"None\")\n",
    "#FireplaceQu: data description says NA means \"no fireplace\"\n",
    "combined_data[\"FireplaceQu\"] = combined_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "\n",
    "# The data description file describes a pool quality with a missing value to mean no pool, therefore we can fill this value in with \n",
    "# none\n",
    "combined_data[\"PoolQC\"] = combined_data[\"PoolQC\"].fillna(\"None\")\n",
    "\n",
    "#GarageType, GarageFinish, GarageQual and GarageCond: Each of these with a missing value means that there is no garage\n",
    "garage = [\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]\n",
    "for column in garage:\n",
    "    combined_data[column] = combined_data[column].fillna(\"None\")\n",
    "    \n",
    "#GarageYrBlt, GarageArea and GarageCars blank values should be replaced with 0\n",
    "for column in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
    "    combined_data[column] = combined_data[column].fillna(0)\n",
    "    \n",
    "\n",
    "# NA in any of the below columns means no basement, therefore the SF will be 0\n",
    "for column in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n",
    "    combined_data[column] = combined_data[column].fillna(0)\n",
    "\n",
    "# NA for any of the below columns means no basement, therefore can be filled with the value 0\n",
    "for column in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n",
    "    combined_data[column] = combined_data[column].fillna(\"None\")\n",
    "    \n",
    "    \n",
    "# MasVnrType with value of NA means none\n",
    "combined_data[\"MasVnrType\"] = combined_data[\"MasVnrType\"].fillna(\"None\")\n",
    "# MasVnrArea of 0 means no MasVnr\n",
    "combined_data[\"MasVnrArea\"] = combined_data[\"MasVnrArea\"].fillna(0)\n",
    "\n",
    "\n",
    "combined_data[\"MSZoning\"] = combined_data[\"MSZoning\"].fillna(combined_data['MSZoning'].mode()[0])\n",
    "\n",
    "\n",
    "# Assumes NA value for Utilities means has all utilities (AllPub)\n",
    "combined_data[\"Utilities\"] = combined_data[\"Utilities\"].fillna(\"AllPub\")\n",
    "\n",
    "#Group by neighborhood and fill in missing value by the mean value of all other houses in the same neighborhood\n",
    "combined_data[\"LotFrontage\"] = combined_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "        lambda x: x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "# Assumes that if no defects are noticed with the house that its functionality\n",
    "# must be typical\n",
    "combined_data[\"Functional\"] = combined_data[\"Functional\"].fillna(\"Typ\")\n",
    "\n",
    "combined_data[\"Electrical\"] = combined_data[\"Electrical\"].fillna(combined_data['Electrical'].mode()[0])\n",
    "\n",
    "combined_data[\"KitchenQual\"] = combined_data[\"KitchenQual\"].fillna(combined_data['KitchenQual'].mode()[0])\n",
    "\n",
    "combined_data[\"Exterior1st\"] = combined_data[\"Exterior1st\"].fillna(combined_data['Exterior1st'].mode()[0])\n",
    "combined_data[\"Exterior2nd\"] = combined_data[\"Exterior2nd\"].fillna(combined_data['Exterior2nd'].mode()[0])\n",
    "\n",
    "\n",
    "combined_data[\"SaleType\"] = combined_data[\"SaleType\"].fillna(combined_data['SaleType'].mode()[0])\n",
    "\n",
    "combined_data[\"MSSubClass\"] = combined_data[\"MSSubClass\"].fillna(\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determines the missing values from the test dataset\n",
    "# and shows which columns have at least one missing value.\n",
    "total = combined_data.isnull().sum().sort_values(ascending = False) [combined_data.isnull().sum().sort_values(ascending = False) != 0]\n",
    "percent = round(combined_data.isnull().sum().sort_values(ascending = False)/len(combined_data)*100,2)[round(combined_data.isnull().sum().sort_values(ascending = False)/len(combined_data)*100,2) != 0]\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['MSSubClass'] = combined_data['MSSubClass'].astype('category')\n",
    "\n",
    "combined_data['OverallCond'] = combined_data['OverallCond'].astype('category')\n",
    "combined_data['OverallQual'] = combined_data['OverallQual'].astype('category')\n",
    "\n",
    "\n",
    "combined_data['YrSold'] = combined_data['YrSold'].astype('category')\n",
    "combined_data['MoSold'] = combined_data['MoSold'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "7k7-9cIIVG-2",
    "outputId": "ead0ac3d-8d76-4fb1-fc2b-213b1b52631b",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "categorical_data = ['FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold']\n",
    "\n",
    "print(combined_data.shape)\n",
    "for i in categorical_data:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(combined_data[i].values))\n",
    "    combined_data[i] = encoder.transform(list(combined_data[i].values))\n",
    "print(combined_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.get_dummies(combined_data)\n",
    "print(combined_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "EdWyFV1NWHBT",
    "outputId": "e23ba1a7-f74b-4a00-f6ab-2bd5aa549c00",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = combined_data.xs(\"train\"), combined_data.xs(\"test\")\n",
    "\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)\n",
    "\n",
    "X = train_dataset\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X,y_train, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the machine learning model linear regression\n",
    "model =  LinearRegression()\n",
    "\n",
    "# train our model with the selected training features features.\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "#Test MAE of dataset with validation data\n",
    "pred_val_y = model.predict(val_X)\n",
    "\n",
    "mae = mean_absolute_error(pred_val_y, val_y)\n",
    "\n",
    "print(\"The mean average error value is:\", mae)\n",
    "\n",
    "# Predict data based on our model\n",
    "final_predict = np.expm1(model.predict(test_dataset))\n",
    "\n",
    "\n",
    "pd.DataFrame({'Id': test_ID, 'SalePrice': final_predict}).to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if IN_COLAB and mounted:\n",
    "     drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing cross validation\n",
    "#the column names' array values should be repalces by the final variables which are going to be used in the model\n",
    "columns = [\"LotArea\", \"FullBath\", \"GrLivArea\", \"FullBath\", \"Fireplaces\"]\n",
    "X = train_dataset[columns]\n",
    "\n",
    "#we already have y_train which stores all the sale prices \n",
    "\n",
    "pipe = make_pipeline(SimpleImputer(), RandomForestRegressor())\n",
    "\n",
    "#getting the scores\n",
    "result = cross_val_score(pipe, X, y_train, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"The scores from the cross-validation process are:\", result)\n",
    "print(\"Average score:\", -1*result.mean())\n",
    "\n",
    "#in this case, when i used variable cattegories picked at random, the score is very low; this is due to the model overfitting the data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TeamChallenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
